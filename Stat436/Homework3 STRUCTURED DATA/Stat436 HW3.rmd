---
title: "Stat 436 Homework-3"
author: "Peiyuan Li"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
    fig_width: 6
    fig_height: 4
    df_print: paged
  pdf_document:
    toc: true
---


```{r}
#Hide unnecessary output and warning message such as library(tidyverse)
knitr::opts_chunk$set(warnings = FALSE, message = FALSE)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(sf)
library(raster)
library(tmap)
```



<br/><br/><br/>

<h2>Coding 1 Bike Demand [3 points]</h2>

<h3> This problem asks you to visualize a dataset of hourly bikeshare demand: </h3>


```{r}
bike_data <- read_csv("https://uwmadison.box.com/shared/static/f16jmkkskylfl1hnd5rpslzduja929g2.csv")
head(bike_data)
```

<h3>a.Make a line plot of bike demand (count) by hour, faceted out across the 7 days of the week
(weekday)</h3>

```{r}

#Select required data and rename them properly
q1 <- bike_data %>%
  dplyr::select(weekday, hr, count) %>%
  mutate(weekday = factor(weekday, levels = 0:6, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>%
  group_by(hr, weekday) %>%
  summarise(summary = sum(count))


# Plotting with geom_line
ggplot(q1, aes(x = hr, y = summary)) + # Ensure that we group by weekday for the line plot
  geom_line(aes(color = weekday)) + 
  facet_wrap(~weekday) + # Facets for each weekday
  labs(x = "Hour of the Day", y = "Count of Bike Rentals", title = "Bike Demand by Hour Across the Week") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5)) # Center the title
```

<h3>b.Create a new summary data.frame giving the 25 and 75 percent quantiles of demand (count) for
each hour (hr) by day of the week (weekday) combination, separately within each year (yr) that
the data was collected</h3>

```{r}
q2 <- bike_data %>%
  mutate(yr = yr + 2011) %>%
  mutate(weekday = factor(weekday, levels = 0:6, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>%
  group_by(yr, weekday, hr) %>%
  summarise(
    Q25 = quantile(count, 0.25),
    Q75 = quantile(count, 0.75)
  ) %>%
  rename(year = yr, hour = hr) # Renaming the columns
q2
```

<h3>c.Using a ribbon plot, overlay the quantities from (b) onto your plot from part (a). Use color to
distinguish between the ribbons for the first and second year that the data were collected</h3>
```{r}
# Merge the summary data with the quantile data.
# Since q2 contains summaries for each year, we will need to join this with q1 based on hr and weekday.
# Assuming that q1 has been grouped and summarised without the year, we will need to account for this.

# First, let's correct the renaming issue in the q2 data frame:
q1 <- q1 %>%
  rename(hour = hr) # Rename hr to hour

# Now let's merge q1 and q2
q3 <- q1 %>%
  ungroup() %>% # Make sure q1 is not grouped
  left_join(q2, by = c("weekday", "hour")) # Join q2 data to q1 data

# Plotting with geom_ribbon and geom_line
ggplot(q3, aes(x = hour, y = summary)) +
  geom_ribbon(aes(ymin = Q25, ymax = Q75, fill = as.factor(year)), alpha = 0.8) +
  geom_line(aes(color = weekday)) + 
  facet_wrap(~weekday) + # Facets for each weekday
  scale_fill_manual(values = c("blue", "red"), name = "Year") + # Set manual colors for the ribbon based on year
  labs(x = "Hour of the Day", y = "Count of Bike Rentals", title = "Bike Demand by Hour Across the Week") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title and move legend to bottom
```

<h3>d.Provide a brief description of some takeaways from the final visualization</h3>

<p>Demand Patterns: There are clear patterns of demand based on the time of day. Demand peaks typically occur during what would be expected to be rush hours, suggesting that bike rentals are possibly used for commuting to and from work or school.</p>

<p>Weekday vs. Weekend: The patterns for weekdays and weekends are noticeably different. Weekends (Saturday and Sunday) tend to have a single broad peak, which could indicate leisure or errand activities that are not bound to the strict schedules of a workday.</p>

<p>Quantile Ranges: The quantile ribbons show that there is variability in the demand. The width of the ribbons indicates the degree of consistency in bike demand. A narrow ribbon suggests that the demand is fairly consistent at that time, whereas a wider ribbon suggests more variability.</p>

<p>Yearly Comparison: The use of different colors for ribbons from different years allows us to compare demand across years. For example, if the red ribbon (for 2011) consistently lies above or below the blue ribbon (for 2012) at certain times, this could indicate a change in usage patterns or bike rental availability from one year to the next.</p>



<h2>Coding 2 & Discussion Geospatial Datasets [3 points]</h2>

<h3>a.NYC Building Footprints</h3>
```{r}
nyc_sf <-read_sf("https://uwmadison.box.com/shared/static/qfmrp9srsoq0a7oj0e7xmgu5spojr33e.geojson")
nyc_sf %>%dplyr::select(geometry)
```
<p>The nyc building is a .geojson file, it implies that the data is in a vector format. It contains geometries of type MultiPolygon because it is presented as <S3: sfc_MULTIPOLYGON></p>



<h3>b.Africa Population 2020</h3>

```{r}
africa_pop <- raster("https://github.com/krisrs1128/stat436_s24/raw/main/data/afripop2020.tif")
africa_pop
```
<p>This is a raster data since it stores in standard TIFF file, and it is easy to see it when we read with R has a "RasterLayer" class information</p>



<h3>c.Himalayan Glacial Lakes</h3>
```{r}
himalayan_lakes <- read_sf("https://raw.githubusercontent.com/krisrs1128/stat436_s23/main/data/GL_3basins_2015.topojson")
himalayan_lakes %>% dplyr::select(geometry)
```
<p>The Himalayan Glacial Lakes is a .topojson file, it implies that the data is in a vector format. It contains geometries of type Polygon because its gemoetry is presented as <S3: sfc_POLYGON></p>



<h3> d.Wisconsin EV Charging</h3>

```{r}
wi_ev_charging <- read_sf("https://raw.githubusercontent.com/krisrs1128/stat436_s24/main/data/ev.geojson")
wi_ev_charging %>% dplyr::select(geometry)
```
<p>The Himalayan Glacial Lakes is a .geojson file, it implies that the data is in a vector format. It contains geometries of type Point because its gemoetry is presented as <S3: sfc_POINT> </p>


<h3> e.Zion Elevation</h3>

```{r}
zion_evl <- raster("https://github.com/krisrs1128/stat436_s24/raw/main/data/landsat.tif")
zion_evl
```
<p>This is a raster data since it stores in standard TIFF file, and it is easy to see it when we read with R has a "RasterLayer" class information</p>


<h3> f.Visualize one of these datasets</h3>
<p> I will visualize e.Zion Elevation for question f </p>
```{r}
tm_shape(zion_evl) +
  tm_raster() 
```


<h2>Coding3 Political Book Recommendations [4 points]</h2>
<p> [Political Book Recommendations] In this problem, we’ll study a network dataset of Amazon bestselling US Politics books. Books are linked by an edge if they appeared together in the recommendations (“customers who bought this book also bought these other books”).</p>

<h3>a.The code below reads in the edges and nodes associated with the network. The edges dataset only
contains IDs of co-recommended books, while the nodes data includes attributes associated with
each book. Build a tbl_graph object to store the graph.</h3>


```{r}
library(tidygraph)
edge_data_path <- "https://raw.githubusercontent.com/krisrs1128/stat992_f23/6c4130bddbdfc9ef90537c794cdca47773643752/activities/week10/political-books-edges.csv"
node_data_path <- "https://github.com/krisrs1128/stat992_f23/raw/6c4130bddbdfc9ef90537c794cdca47773643752/activities/week10/political-books-nodes.csv"
edges <- read_csv(edge_data_path, col_types = "cci")
nodes <- read_csv(node_data_path, col_types = "ccc")
# Building the tbl_graph
book_graph <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)
book_graph
```


<h3>b.Use the result from part (a) to visualize the network as a node-link diagram. Include the book’s
title in the node label, and shade in the node according to political ideology.</h3>

```{r}
library(ggraph)
ggraph(book_graph, layout = "kk") + 
  geom_edge_link(width = 0.1) + 
  geom_node_point(aes(color = political_ideology))+
  geom_node_label(aes(label = label), size = 1.1, repel = TRUE)
```



<h3>c.Create the analogous adjacency matrix visualization. Provide examples of visual queries that are
easy to answer using one encoding but not the other (i.e., what is easy to see in the node-link view
vs. what is easy to see in the adjacency matrix).</h3>

<p>I tried to add label on y and x axis, but there are so many data point and overlapping so i only have clean matrix graph instead of a labeled one</p>
```{r}
ggraph(book_graph, "matrix") +
  geom_edge_tile(mirror = TRUE) + #its not directed so mirror, its ok not mirror as well
  coord_fixed() #make sure grid are square
```



<p>What is easy to see in the node-link view: It's easier to trace the connection between two nodes.</p>
<p>Example Query: Which books are recommended together with the book "All the Shah's Men"?</p>


<p>What is easy to see in the adjacency matrix: It's easier to find the density of connections around each nodes.</p>
<p>Example Query: Are there any books that are outliers, being recommended with only one or no other books?</p>






<h2>Coding 4 Colony Collapse, parts a – c [4 points]<h2>
<p>. [Colony Collapse] In this problem, we will study a dataset describing factors that might be leading
to colony collapse disorder among bees. Since there are multiple stressors for each state × timepoint
combination, we will use clustering to create a summarized “ecological stress” profile.</p>


<h3>a. The code below reads in data from its original Tidy Tuesday source and replaces NAs with
0’s. We will summarize each state × timepoint combination by its profile of stress_pct across
each stressor. Reshape the data so that stressor appears along columns, with the associated
percentage contained within the table. Each row is therefore a profile of the amount of different
stressors at a particular location and time.<h3>

```{r}
stressor <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/stressor.csv') %>%
mutate(stress_pct = replace_na(stress_pct, 0))
# Reshape the data to have stressor types as columns
stressor_wide <- stressor %>%
  pivot_wider(
    names_from = stressor,        # Create new column names from the 'stressor' values
    values_from = stress_pct,     # Fill the columns with values from the 'stress_pct'
  )

stressor_wide
```


<h3>b.Apply K-means to the profiles constructed in part (a). You may choose K. Visualize and briefly
interpret the resulting centroids.</h3>


```{r}
library("tidymodels")
library("superheat")
kclust <- stressor_wide %>%  
  select(-year, -months, -state) %>%  #filter out non-numeric columns
  kmeans(centers = 10) #set 10 centorids

stressor_wide <- augment(kclust, stressor_wide) # creates column ".cluster" with cluster label
stressor_wide %>%
  select(year, months, state, .cluster) %>%
  arrange(.cluster)

# Assuming kclust is the result of your kmeans
centroids <- kclust$centers  # Extract centroids
print(centroids)
```