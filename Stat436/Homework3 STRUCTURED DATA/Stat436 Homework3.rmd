---
title: "Stat 436 Homework-3"
author: "Peiyuan Li"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
    fig_width: 6
    fig_height: 4
    df_print: paged
  pdf_document:
    toc: true
---
```{r}
#Hide unnecessary output and warning message such as library(tidyverse)
knitr::opts_chunk$set(warnings = FALSE, message = FALSE)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(sf)
library(raster)
library(tmap)
library(tidymodels)
library(shiny)
```

<br/><br/><br/>

<h2>Coding 1 Bike Demand [3 points]</h2>

<p>This problem asks you to visualize a dataset of hourly bikeshare demand:</p>

```{r}
bike_data <- read_csv("https://uwmadison.box.com/shared/static/f16jmkkskylfl1hnd5rpslzduja929g2.csv")
head(bike_data)
```

<h3>a.Make a line plot of bike demand (count) by hour, faceted out
across the 7 days of the week (weekday)</h3>

```{r}


# Convert 'weekday' to a factor if it's not already
q1 <- bike_data %>%
  mutate(yr = yr + 2011) %>%
  mutate(weekday = factor(weekday, levels = 0:6, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>%
  rename(year = yr, hour = hr) # Renaming the columns

# Plotting with geom_line
ggplot(q1, aes(x = hour, y = count, group = dteday)) + 
  geom_line(aes(color = weekday)) + #color lines by weekday
  scale_color_manual(values = c("red", "orange", "yellow", "green", "blue", "#4b0082", "violet"))+ #fix the color 
  facet_wrap(~weekday)+
  labs(x = "Hour of the Day", 
       y = "Count of Bike Rentals", 
       title = "Bike Demand by Hour Across the Week") + #adding labs
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5))# Center the title
```

<h3>b.Create a new summary data.frame giving the 25 and 75 percent
quantiles of demand (count) for each hour (hr) by day of the week
(weekday) combination, separately within each year (yr) that the data
was collected</h3>

```{r}
q2 <- q1 %>%
  group_by(year, weekday, hour) %>%
  summarise(
    Q25 = quantile(count, 0.25),
    Q75 = quantile(count, 0.75)
  ) 
q2
```

<h3>c.Using a ribbon plot, overlay the quantities from (b) onto your
plot from part (a). Use color to distinguish between the ribbons for the
first and second year that the data were collected</h3>

```{r}
# Merge the summary data with the quantile data.
# Since q2 contains summaries for each year, we will need to join this with q1 based on hr and weekday.
# Assuming that q1 has been grouped and summarised without the year, we will need to account for this.

# Now merge q1 and q2
q3 <- q1 %>%
  ungroup() %>% # Make sure q1 is not grouped
  left_join(q2, by = c("weekday", "hour", "year")) # Join q2 data to q1 data

# Plotting with geom_line
ggplot(q3, aes(x = hour, y = count, group = dteday)) +
  geom_line(aes(color = weekday)) + #color lines by weekday
  geom_ribbon(aes(ymin = Q25, ymax = Q75, fill = as.factor(year)), alpha = 0.8) +
  scale_color_manual(values = c("red", "orange", "yellow", "green", "blue", "#4b0082", "violet"))+ #fix the color 
  facet_wrap(~weekday)+
  labs(x = "Hour of the Day", 
       y = "Count of Bike Rentals", 
       title = "Bike Demand by Hour Across the Week") + #adding labs
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5))# Center the title
```

<h3>d.Provide a brief description of some takeaways from the final
visualization</h3>

<p>Demand Patterns of Weekday: There are clear patterns of demand based on the time
of day. Demand peaks typically occur during what would be expected to be
rush hours (8-10 am, 4-6 pm), suggesting that bike rentals are possibly used for commuting
to and from work or school.</p>

<p>Weekday vs. Weekend: The patterns for weekdays and weekends are
noticeably different. Weekends (Saturday and Sunday) tend to have a
single broad peak, which could indicate leisure activities compare to the demand patterns  
restricted by schedules of a workday.</p>


<p>Yearly Comparison: For each weekday we can compare the two different ribbon,
for 2011 and 2012. Its noticeably that 2012's Q25-Q75 range is always stay beyond 2011's Q25-Q75 range. 
This could indicate a change in usage patterns or bike rental availability from 2011 year to 2012 
lead to this huge increase bike demand.</p>




<h2>Coding 2 & Discussion 1 Geospatial Datasets [3 points]</h2>

<p>For each of the datasets below, specify whether it is in a vector or raster data
format. If it is in a vector data format, explain which types of geometries it contains (e.g., a point or
linestring). Explain your reasoning.</p>

<h3>a.NYC Building Footprints</h3>

```{r}
nyc_sf <-read_sf("https://uwmadison.box.com/shared/static/qfmrp9srsoq0a7oj0e7xmgu5spojr33e.geojson")
nyc_sf %>%dplyr::select(geometry)
```

<p>The nyc building is a .geojson file, it implies that the data is in a
vector format. It contains geometries of type MultiPolygon because it is
presented as \<S3: sfc_MULTIPOLYGON\></p>

<h3>b.Africa Population 2020</h3>

```{r}
africa_pop <- raster("https://github.com/krisrs1128/stat436_s24/raw/main/data/afripop2020.tif")
africa_pop
```

<p>This is a raster data since it stores in standard TIFF file, and it
is easy to see it when we read with R has a "RasterLayer" class
information</p>

<h3>c.Himalayan Glacial Lakes</h3>

```{r}
himalayan_lakes <- read_sf("https://raw.githubusercontent.com/krisrs1128/stat436_s23/main/data/GL_3basins_2015.topojson")
himalayan_lakes %>% dplyr::select(geometry)
```

<p>The Himalayan Glacial Lakes is a .topojson file, it implies that the
data is in a vector format. It contains geometries of type Polygon
because its gemoetry is presented as \<S3: sfc_POLYGON\></p>

<h3>d.Wisconsin EV Charging</h3>

```{r}
wi_ev_charging <- read_sf("https://raw.githubusercontent.com/krisrs1128/stat436_s24/main/data/ev.geojson")
wi_ev_charging %>% dplyr::select(geometry)
```

<p>The Wisconsin EV Charging is a .geojson file, it implies that the
data is in a vector format. It contains geometries of type Point because
its gemoetry is presented as \<S3: sfc_POINT\></p>

<h3>e.Zion Elevation</h3>

```{r}
zion_evl <- raster("https://github.com/krisrs1128/stat436_s24/raw/main/data/landsat.tif")
zion_evl
```

<p>This is a raster data since it stores in standard TIFF file, and it
is easy to see it when we read with R has a "RasterLayer" class
information</p>

<h3>f.Visualize one of these datasets</h3>

<p>I will visualize e.Zion Elevation for question f</p>

```{r}
tm_shape(zion_evl) +
  tm_raster() 
```

<h2>Coding 3 Political Book Recommendations [4 points]</h2>

<p>[Political Book Recommendations] In this problem, we’ll study a
network dataset of Amazon bestselling US Politics books. Books are
linked by an edge if they appeared together in the recommendations
(“customers who bought this book also bought these other books”).</p>

<h3>a.The code below reads in the edges and nodes associated with the
network. The edges dataset only contains IDs of co-recommended books,
while the nodes data includes attributes associated with each book.
Build a tbl_graph object to store the graph.</h3>

```{r}
library(tidygraph)
edge_data_path <- "https://raw.githubusercontent.com/krisrs1128/stat992_f23/6c4130bddbdfc9ef90537c794cdca47773643752/activities/week10/political-books-edges.csv"
node_data_path <- "https://github.com/krisrs1128/stat992_f23/raw/6c4130bddbdfc9ef90537c794cdca47773643752/activities/week10/political-books-nodes.csv"
edges <- read_csv(edge_data_path, col_types = "cci")
nodes <- read_csv(node_data_path, col_types = "ccc")
# Building the tbl_graph
book_graph <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)
book_graph
```

<h3>b.Use the result from part (a) to visualize the network as a
node-link diagram. Include the book’s title in the node label, and shade
in the node according to political ideology.</h3>

```{r}
library(ggraph)
ggraph(book_graph, layout = "kk") + 
  geom_edge_link(width = 0.1) + 
  geom_node_point(aes(color = political_ideology))+
  geom_node_label(aes(label = label), size = 1.1, repel = TRUE)
```

<h3>c.Create the analogous adjacency matrix visualization. Provide
examples of visual queries that are easy to answer using one encoding
but not the other (i.e., what is easy to see in the node-link view vs.
what is easy to see in the adjacency matrix).</h3>

<p>I tried to add label on y and x axis, but there are so many data
point and overlapping so i only have clean matrix graph instead of a
labeled one</p>

```{r}
ggraph(book_graph, "matrix") +
  geom_node_point(aes(col = political_ideology), x = -1) +
  geom_node_point(aes(col = political_ideology), y = 1) +
  geom_edge_tile(mirror = TRUE) + #its not directed so mirror, its ok not mirror as well
  coord_fixed() #make sure grid are square
```

<p>What is easy to see in the node-link view: It's easier to trace the
connection between two specific nodes. (ie linked or not)</p>

<p>Example Query: Which books are recommended together with the book
"All the Shah's Men"?</p>

<p>What is easy to see in the adjacency matrix: It's easier to find the
density of connections around each nodes.</p>

<p>Example Query: Are there any books that are outliers, being
recommended with only one or no other books?</p>



<h2>Coding 4 Colony Collapse, parts a – c [4 points]</h2>

<p> In this problem, we will study a dataset describing factors that might be leading to colony collapse disorder among bees. Since there are multiple stressors for each state × timepoint combination, we will use clustering to create a summarized “ecological stress” profile.</p>

<h3>a.The code below reads in data from its original Tidy Tuesday source
    and replaces NAs with 0’s. We will summarize each state × timepoint
    combination by its profile of stress_pct across each stressor.
    Reshape the data so that stressor appears along columns, with the
    associated percentage contained within the table. Each row is
    therefore a profile of the amount of different stressors at a
    particular location and time.
</h3>

```{r}
stressor <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/stressor.csv') %>%
mutate(stress_pct = replace_na(stress_pct, 0))
# Reshape the data to have stressor types as columns
stressor_wide <- stressor %>%
  pivot_wider(
    names_from = stressor,        # Create new column names from the 'stressor' values
    values_from = stress_pct,     # Fill the columns with values from the 'stress_pct'
  )

stressor_wide
```

<h3>b.Apply K-means to the profiles constructed in part (a). You may
choose K. Visualize and briefly interpret the resulting centroids.</h3>

```{r}
k <- 10  #set 10 centorids

#Apply kmean on stressor wide
kclust <- stressor_wide %>%  
  select(-year, -months, -state) %>%  #filter out non-numeric columns
  kmeans(centers = k)

# creates column ".cluster" with cluster label
stressor_wide <- augment(kclust, stressor_wide) 

# Extract centroids after tidy kclust
centroids <- tidy(kclust) %>% select(1:6, cluster)

# Pivot the centroids longer for  data preparation
long_centroids <- centroids %>%
  pivot_longer(cols = -cluster, names_to = "Attribute", values_to = "Value")

# Use ggplot2 to create the bar plot
ggplot(long_centroids, aes(x = factor(cluster), y = Value, fill = Attribute)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  labs(x = "Cluster", y = "Value", fill = "Attribute")
```
<p>Interpretation: </p>
<p>If one feature has particularly high or low values across clusters,
it could be an important distinguishing feature for clustering.</p>

<p>Thus, "Varroa mites", "other pests/parasites", and "disease" are
relatively powerful in determine a cluster.</p>

<h3>c.Design and implement a visualization to show the change in cluster
    memberships over time. Briefly interpret your results in context.</h3>

```{r}
#sort stressor wide based on their cluster and select only times, state and .cluster
stressor_wide <- stressor_wide %>%
  select(year, months, state, .cluster)

# Create a 'time' variable by pasting 'year' and 'months' together
stressor_wide <- stressor_wide %>%
  mutate(time = paste(year, months)) %>%
  mutate(time = factor(time, levels = unique(time))) # Convert to a factor to maintain order

# Plot .cluster by time for each state, with facets for each state
ggplot(stressor_wide, aes(x = time, y = .cluster)) +
  geom_point(aes(col = state)) + 
  facet_wrap(~ state, ncol = 10) + # Facets by state
  theme_minimal() +
  labs(x = "Time", 
       y = "Cluster", 
       title = "Cluster Memberships Over Time") +
  theme(
    axis.text.x = element_blank(), # Hide y axis text, time too long
    legend.position = "none", #Hide color legend
    axis.text.y = element_text(size = 5), #modify size for repel
    plot.title = element_text(hjust = 0.5), # Center the plot title
    ) 
```

<p>In the results of my plot, most states are relatively stable,
generally fluctuating between 2 clusters. This indicates that the stress
percentage of most states are quite steady, which points to a broader
trend. Examples of such states include Wyoming, Maine, and Oklahoma.</p>

<p>However, there are some states with significant variations,
oscillating across multiple clusters without a discernible trend, such
as Wisconsin and North Carolina. This may suggest that there are factors
influencing the stress percentages in these states that change over
time.</p>

<h2>Discussion 2: Olympics Interactive App [3 points]</h2>

<p>The code below sets up a Shiny app for interactively visualizing
athlete weight and heights in the 2012 London Olympics. We would like to
have an interactive scatterplot of Weight vs. Height, cm that updates
which points (athletes) are highlighted depending on which sports have
been selected by a dropdown menu. Code for generating the scatterplot is
provided in the function scatterplot.</p>

<h3>a.Provide server code which would allow the scatterplot to update the highlighted athletes depending on the currently selected sports. </h3>

```{r}
olympics <- read_csv("https://uwmadison.box.com/shared/static/rzw8h2x6dp5693gdbpgxaf2koqijo12l.csv")
#' Scatterplot with highlighted points
#'
#' Assumes a column in df called "selected" saying whether points should be
#' larger / darker
scatterplot <- function(df) {
  ggplot(df) +
    geom_point(aes(Weight, `Height, cm`,alpha = as.numeric(selected),size = as.numeric(selected))) +
  scale_alpha(range = c(0.05, .8)) +
  scale_size(range = c(0.1, 1))
}
ui <- fluidPage(
  selectInput("dropdown", "Select a Sport", choices = unique(olympics$Sport), multiple = TRUE),
  plotOutput("scatterplot")
)
server <- function(input, output) {
  # Answer of question a starts here:
  # Reactive expression to filter data based on selected sports
  filtered_data <- reactive({
    if (length(input$dropdown) == 0) {
      olympics$selected <- FALSE
    } else {
      olympics$selected <- olympics$Sport %in% input$dropdown
    }
    olympics
  })

  # Render the scatterplot based on filtered data
  output$scatterplot <- renderPlot({
    scatterplot(filtered_data())
  })
}
```

<h3>b.We have been asked to also print a table of the selected athletes.</h3>

```{r}
olympics <- read_csv("https://uwmadison.box.com/shared/static/rzw8h2x6dp5693gdbpgxaf2koqijo12l.csv")
#' Scatterplot with highlighted points
#'
#' Assumes a column in df called "selected" saying whether points should be
#' larger / darker
scatterplot <- function(df) {
  ggplot(df) +
  geom_point(aes(Weight, `Height, cm`,alpha = as.numeric(selected),size = as.numeric(selected))) +
  scale_alpha(range = c(0.05, .8)) +
  scale_size(range = c(0.1, 1))
}
ui <- fluidPage(
  selectInput("dropdown", "Select a Sport", choices = unique(olympics$Sport), multiple = TRUE),
  plotOutput("scatterplot"),
  dataTableOutput("table")
)
server <- function(input, output) {
  # Answer of question b starts here:
  # Reactive expression to filter data based on selected sports
  filtered_data <- reactive({
    if (length(input$dropdown) == 0) {
      olympics$selected <- FALSE
    } else {
      olympics$selected <- olympics$Sport %in% input$dropdown
    }
    olympics
  })

  # Render the scatterplot based on filtered data
  output$scatterplot <- renderPlot({
    scatterplot(filtered_data())
  })
  
  # Render the data table for selected athletes
  output$table <- renderDataTable({
    filtered_data() %>%
      filter(selected == TRUE) %>%
      select(Name, Sport, `Height, cm`, Weight)
  })
}
```
<p>Describe changes to your solution to (a) to meet the new requirements. How would you minimize code
duplication? Be as specific as possible.</p>
</br>
<p>I add output\$table using renderDataTable. This outputs a table using
a filtered version of the filtered_data reactive expression. The same
filtered_data expression is used for both the scatterplot and the data
table, which ensures consistency across the two outputs and minimizes
redundant code.</p>
